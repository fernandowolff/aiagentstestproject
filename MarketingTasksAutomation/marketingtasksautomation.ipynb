{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "import os\n",
    "import yaml\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool\n",
    "from crewai import LLM\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': 'config/agents.yaml',\n",
    "    'tasks': 'config/tasks.yaml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"ollama/llama3.2:3b\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 17:30:39,723 - 140281196643392 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "# Creating Agents\n",
    "content_retriever_agent = Agent(\n",
    "  config=agents_config['content_retriever_agent'],\n",
    "  tools=[SerperDevTool(), ScrapeWebsiteTool()],\n",
    "  llm=llm\n",
    ")\n",
    "\n",
    "content_revisor_agent = Agent(\n",
    "  config=agents_config['content_revisor_agent'],\n",
    "  tools=[ScrapeWebsiteTool()],\n",
    "  llm=llm\n",
    ")\n",
    "\n",
    "post_writer_agent = Agent(\n",
    "    config=agents_config['post_writer_agent'],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "content_retrieval = Task(\n",
    "  config=tasks_config['content_retrieval'],\n",
    "  agent=content_retriever_agent\n",
    ")\n",
    "\n",
    "content_revision = Task(\n",
    "  config=tasks_config['content_revision'],\n",
    "  agent=content_revisor_agent\n",
    ")\n",
    "\n",
    "post_writting = Task(\n",
    "    config=tasks_config['post_writting'],\n",
    "    agent=post_writer_agent\n",
    ")\n",
    "\n",
    "# Creating Crew\n",
    "crew = Crew(\n",
    "  agents=[\n",
    "    content_retriever_agent,\n",
    "    content_revisor_agent,\n",
    "    post_writer_agent\n",
    "  ],\n",
    "  tasks=[\n",
    "    content_retrieval,\n",
    "    content_revision,\n",
    "    post_writting,\n",
    "  ],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Retriever\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mSearch and summaryze content from the internet around the subject: What is Ollama?. The summary should be some paragraphs wide and contain from 2 to 4 different sources. Keep track of where each content you retrieved came from, adding the links to the summary, so that it can be inspected later on.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Retriever\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch the internet\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"What is Ollama?\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "\n",
      "Search results: Title: Ollama: Easily run LLMs locally - Klu.ai\n",
      "Link: https://klu.ai/glossary/ollama\n",
      "Snippet: Ollama is a tool that allows you to run open-source large language models (LLMs) locally on your machine. It supports a variety of models, including Llama 2, ...\n",
      "---\n",
      "Title: What is Ollama and how to use it on Windows / Install ... - YouTube\n",
      "Link: https://www.youtube.com/watch?v=Za0Ms8d9fso\n",
      "Snippet: If you want to run Large Language Models or LLMs on your computer, one of the easiest ways to ...\n",
      "---\n",
      "Title: Ollama\n",
      "Link: https://ollama.com/\n",
      "Snippet: Get up and running with large language models. Run Llama 3.2, Phi 3, Mistral, Gemma 2, and other models. Customize and create your own.\n",
      "---\n",
      "Title: Why use Ollama? : r/LocalLLaMA - Reddit\n",
      "Link: https://www.reddit.com/r/LocalLLaMA/comments/1dhyxq8/why_use_ollama/\n",
      "Snippet: Ollama is often very quick at providing all of the quants of a model family on the hub. Their modelfiles they use include the official chat ...\n",
      "---\n",
      "Title: What is Ollama? Everything Important You Should Know\n",
      "Link: https://itsfoss.com/ollama/\n",
      "Snippet: Ollama is a free and open-source tool that lets anyone run open LLMs locally on your system. It supports Linux (Systemd-powered distros), ...\n",
      "---\n",
      "Title: Ollama Explained: Transforming AI Accessibility and Language ...\n",
      "Link: https://www.geeksforgeeks.org/ollama-explained-transforming-ai-accessibility-and-language-processing/\n",
      "Snippet: Ollama is a groundbreaking platform that democratizes access to large language models (LLMs) by enabling users to run them locally on their machines.\n",
      "---\n",
      "Title: Ollama Tutorial: Running LLMs Locally Made Super Simple\n",
      "Link: https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple\n",
      "Snippet: Want to run large language models on your machine? Learn how to do so using Ollama in this quick tutorial.\n",
      "---\n",
      "Title: ollama/ollama: Get up and running with Llama 3.2, Mistral ... - GitHub\n",
      "Link: https://github.com/ollama/ollama\n",
      "Snippet: Ollama is a lightweight, extensible framework for building and running language models on the local machine.\n",
      "---\n",
      "Title: Ollama - LangChain4j\n",
      "Link: https://docs.langchain4j.dev/integrations/language-models/ollama/\n",
      "Snippet: Ollama is an advanced AI tool that allows users to easily set up and run large language models locally (in CPU and GPU modes). With Ollama, users can leverage ...\n",
      "---\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Retriever\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Ollama is a free and open-source tool that lets anyone run open LLMs locally on their system. It supports Linux (Systemd-powered distros), Windows, and macOS. Ollama provides an easy-to-use interface for running large language models, making it accessible to users who may not have the technical expertise or resources to do so otherwise.\n",
      "\n",
      "According to Ollama's official website, the tool allows users to run a variety of models, including Llama 2, Phi 3, Mistral, Gemma 2, and others. Users can also customize and create their own models using Ollama's framework. The platform is designed to be lightweight and extensible, making it easy for users to build and deploy their own language models.\n",
      "\n",
      "One of the key benefits of using Ollama is that it democratizes access to large language models, enabling users to run them locally on their machines rather than relying on cloud-based services. This can be especially useful for individuals or organizations with limited internet connectivity or high latency connections.\n",
      "\n",
      "Ollama has been featured in various media outlets, including YouTube videos and articles on websites such as ItsFoss and GeeksforGeeks. These resources provide more information on how to use Ollama, its features, and its potential applications.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
      "* https://www.youtube.com/watch?v=Za0Ms8d9fso\n",
      "* Ollama.com: Get up and running with large language models\n",
      "* https://ollama.com/\n",
      "* Reddit: r/LocalLLaMA - Why use Ollama?\n",
      "* https://www.reddit.com/r/LocalLLaMA/comments/1dhyxq8/why_use_ollama/\n",
      "* ItsFoss: \"Ollama is a free and open-source tool that lets anyone run open LLMs locally on your system.\"\n",
      "* https://itsfoss.com/ollama/\n",
      "* GeeksforGeeks: \"Ollama is a groundbreaking platform that democratizes access to large language models by enabling users to run them locally on their machines.\"\n",
      "* https://www.geeksforgeeks.org/ollama-explained-transforming-ai-accessibility-and-language-processing/\n",
      "* KDNuggets: \"Ollama Tutorial: Running LLMs Locally Made Super Simple\"\n",
      "* https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple\n",
      "* GitHub: \"Ollama is a lightweight, extensible framework for building and running language models on the local machine.\"\n",
      "* https://github.com/ollama/ollama\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Revisor\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mReview the created content from the Retriever and make sure it presents correct and valuable information around the subject: What is Ollama?. Make use of the referred web sources available in the text to check if the information summarized is correct according to the original online text.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Revisor\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Ollama is a free and open-source tool that lets anyone run open LLMs locally on their system. It supports Linux (Systemd-powered distros), Windows, and macOS. Ollama provides an easy-to-use interface for running large language models, making it accessible to users who may not have the technical expertise or resources to do so otherwise.\n",
      "\n",
      "According to Ollama's official website, the tool allows users to run a variety of models, including Llama 2, Phi 3, Mistral, Gemma 2, and others. Users can also customize and create their own models using Ollama's framework. The platform is designed to be lightweight and extensible, making it easy for users to build and deploy their own language models.\n",
      "\n",
      "One of the key benefits of using Ollama is that it democratizes access to large language models, enabling users to run them locally on their machines rather than relying on cloud-based services. This can be especially useful for individuals or organizations with limited internet connectivity or high latency connections.\n",
      "\n",
      "Ollama has been featured in various media outlets, including YouTube videos and articles on websites such as ItsFoss and GeeksforGeeks. These resources provide more information on how to use Ollama, its features, and its potential applications.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
      "* https://www.youtube.com/watch?v=Za0Ms8d9fso\n",
      "* Ollama.com: Get up and running with large language models\n",
      "* https://ollama.com/\n",
      "* Reddit: r/LocalLLaMA - Why use Ollama?\n",
      "* https://www.reddit.com/r/LocalLLaMA/comments/1dhyxq8/why_use_ollama/\n",
      "* ItsFoss: \"Ollama is a free and open-source tool that lets anyone run open LLMs locally on your system.\"\n",
      "* https://itsfoss.com/ollama/\n",
      "* GeeksforGeeks: \"Ollama is a groundbreaking platform that democratizes access to large language models by enabling users to run them locally on their machines.\"\n",
      "* https://www.geeksforgeeks.org/ollama-explained-transforming-ai-accessibility-and-language-processing/\n",
      "* KDNuggets: \"Ollama Tutorial: Running LLMs Locally Made Super Simple\"\n",
      "* https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple\n",
      "* GitHub: \"Ollama is a lightweight, extensible framework for building and running language models on the local machine.\"\n",
      "* https://github.com/ollama/ollama\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPost Writer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCreate a compelling Instagram Post using the content summary provided to you around the subject: What is Ollama?. The posts present the most important information around the subject.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPost Writer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "**Slide 1: What is Ollama?**\n",
      "=====================================\n",
      "\n",
      "**Title:** Unlock Access to Large Language Models with Ollama\n",
      "**Image Suggestion:** A futuristic illustration of a person sitting in front of a computer with a cityscape in the background.\n",
      "\n",
      "Ollama is a free and open-source tool that lets anyone run open LLMs locally on their system. It supports Linux (Systemd-powered distros), Windows, and macOS, making it accessible to users from all over the world.\n",
      "\n",
      "**Source:** Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
      "\n",
      "**Slide 2: Features of Ollama**\n",
      "==========================\n",
      "\n",
      "**Title:** What Can You Do with Ollama?\n",
      "**Image Suggestion:** An infographic showcasing various features of Ollama, such as model selection, customization, and deployment.\n",
      "\n",
      "* Run a variety of models, including Llama 2, Phi 3, Mistral, Gemma 2, and others.\n",
      "* Customize and create your own models using Ollama's framework.\n",
      "* Lightweight and extensible, making it easy to build and deploy language models.\n",
      "\n",
      "**Source:** Ollama.com: Get up and running with large language models\n",
      "\n",
      "**Slide 3: Benefits of Using Ollama**\n",
      "==================================\n",
      "\n",
      "**Title:** Democratizing Access to Large Language Models\n",
      "**Image Suggestion:** A picture of a person working on their laptop in a coffee shop, representing remote work and accessibility.\n",
      "\n",
      "By using Ollama, you can:\n",
      "\n",
      "* Run large language models locally on your machine, reducing dependence on cloud-based services.\n",
      "* Enjoy faster and more stable performance, thanks to the platform's lightweight design.\n",
      "* Support open-source projects and contribute to the development of AI technology.\n",
      "\n",
      "**Source:** Reddit: r/LocalLLaMA - Why use Ollama?\n",
      "\n",
      "Sources:\n",
      "\n",
      "* Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
      "* Ollama.com: Get up and running with large language models\n",
      "* Reddit: r/LocalLLaMA - Why use Ollama?\n",
      "* ItsFoss: \"Ollama is a free and open-source tool that lets anyone run open LLMs locally on your system.\"\n",
      "* GeeksforGeeks: \"Ollama is a groundbreaking platform that democratizes access to large language models by enabling users to run them locally on their machines.\"\n",
      "* KDNuggets: \"Ollama Tutorial: Running LLMs Locally Made Super Simple\"\n",
      "* GitHub: \"Ollama is a lightweight, extensible framework for building and running language models on the local machine.\"\u001b[00m\n",
      "\n",
      "\n",
      "**Slide 1: What is Ollama?**\n",
      "=====================================\n",
      "\n",
      "**Title:** Unlock Access to Large Language Models with Ollama\n",
      "**Image Suggestion:** A futuristic illustration of a person sitting in front of a computer with a cityscape in the background.\n",
      "\n",
      "Ollama is a free and open-source tool that lets anyone run open LLMs locally on their system. It supports Linux (Systemd-powered distros), Windows, and macOS, making it accessible to users from all over the world.\n",
      "\n",
      "**Source:** Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
      "\n",
      "**Slide 2: Features of Ollama**\n",
      "==========================\n",
      "\n",
      "**Title:** What Can You Do with Ollama?\n",
      "**Image Suggestion:** An infographic showcasing various features of Ollama, such as model selection, customization, and deployment.\n",
      "\n",
      "* Run a variety of models, including Llama 2, Phi 3, Mistral, Gemma 2, and others.\n",
      "* Customize and create your own models using Ollama's framework.\n",
      "* Lightweight and extensible, making it easy to build and deploy language models.\n",
      "\n",
      "**Source:** Ollama.com: Get up and running with large language models\n",
      "\n",
      "**Slide 3: Benefits of Using Ollama**\n",
      "==================================\n",
      "\n",
      "**Title:** Democratizing Access to Large Language Models\n",
      "**Image Suggestion:** A picture of a person working on their laptop in a coffee shop, representing remote work and accessibility.\n",
      "\n",
      "By using Ollama, you can:\n",
      "\n",
      "* Run large language models locally on your machine, reducing dependence on cloud-based services.\n",
      "* Enjoy faster and more stable performance, thanks to the platform's lightweight design.\n",
      "* Support open-source projects and contribute to the development of AI technology.\n",
      "\n",
      "**Source:** Reddit: r/LocalLLaMA - Why use Ollama?\n",
      "\n",
      "Sources:\n",
      "\n",
      "* Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
      "* Ollama.com: Get up and running with large language models\n",
      "* Reddit: r/LocalLLaMA - Why use Ollama?\n",
      "* ItsFoss: \"Ollama is a free and open-source tool that lets anyone run open LLMs locally on your system.\"\n",
      "* GeeksforGeeks: \"Ollama is a groundbreaking platform that democratizes access to large language models by enabling users to run them locally on their machines.\"\n",
      "* KDNuggets: \"Ollama Tutorial: Running LLMs Locally Made Super Simple\"\n",
      "* GitHub: \"Ollama is a lightweight, extensible framework for building and running language models on the local machine.\"\n"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff(inputs={\n",
    "  'subject': 'What is Ollama?'\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Slide 1: What is Ollama?**\n",
       "=====================================\n",
       "\n",
       "**Title:** Unlock Access to Large Language Models with Ollama\n",
       "**Image Suggestion:** A futuristic illustration of a person sitting in front of a computer with a cityscape in the background.\n",
       "\n",
       "Ollama is a free and open-source tool that lets anyone run open LLMs locally on their system. It supports Linux (Systemd-powered distros), Windows, and macOS, making it accessible to users from all over the world.\n",
       "\n",
       "**Source:** Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
       "\n",
       "**Slide 2: Features of Ollama**\n",
       "==========================\n",
       "\n",
       "**Title:** What Can You Do with Ollama?\n",
       "**Image Suggestion:** An infographic showcasing various features of Ollama, such as model selection, customization, and deployment.\n",
       "\n",
       "* Run a variety of models, including Llama 2, Phi 3, Mistral, Gemma 2, and others.\n",
       "* Customize and create your own models using Ollama's framework.\n",
       "* Lightweight and extensible, making it easy to build and deploy language models.\n",
       "\n",
       "**Source:** Ollama.com: Get up and running with large language models\n",
       "\n",
       "**Slide 3: Benefits of Using Ollama**\n",
       "==================================\n",
       "\n",
       "**Title:** Democratizing Access to Large Language Models\n",
       "**Image Suggestion:** A picture of a person working on their laptop in a coffee shop, representing remote work and accessibility.\n",
       "\n",
       "By using Ollama, you can:\n",
       "\n",
       "* Run large language models locally on your machine, reducing dependence on cloud-based services.\n",
       "* Enjoy faster and more stable performance, thanks to the platform's lightweight design.\n",
       "* Support open-source projects and contribute to the development of AI technology.\n",
       "\n",
       "**Source:** Reddit: r/LocalLLaMA - Why use Ollama?\n",
       "\n",
       "Sources:\n",
       "\n",
       "* Klu.ai: \"Ollama: Easily run LLMs locally\"\n",
       "* Ollama.com: Get up and running with large language models\n",
       "* Reddit: r/LocalLLaMA - Why use Ollama?\n",
       "* ItsFoss: \"Ollama is a free and open-source tool that lets anyone run open LLMs locally on your system.\"\n",
       "* GeeksforGeeks: \"Ollama is a groundbreaking platform that democratizes access to large language models by enabling users to run them locally on their machines.\"\n",
       "* KDNuggets: \"Ollama Tutorial: Running LLMs Locally Made Super Simple\"\n",
       "* GitHub: \"Ollama is a lightweight, extensible framework for building and running language models on the local machine.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(result.raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 ('aiagentstestproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b19f0efc1bc62e1997ac502f2bdeb23988816b4d8ebcf71ea1cd660832878468"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
